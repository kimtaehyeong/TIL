# 면접질문 모음

## 
* 표본(sample)과 모집단(population) : 표본과 모집단은 통계쪽에서 가장 기본적으로 사용되는 용어로, 표본은 샘플이라는 표현을 많이 사용한다. 즉, 샘플이라는걸 많이 사용하는것처럼 전체의 데이터에 일부분이나 결과들의 집합이라고 생각할 수 있으며 모집단은 집단의 전체를 의미하는데 연구자가 알고 싶어하는 집단 전체를 의미하고 대한민국 남자와 여자의 평균 키를 알고 싶어하면 모집단은 대한민국 모든 남자 여자의 키가되고, 각각 하나의 데이터에 대해서는 sample이 된다.
  
* 왜 eigenvector & eigenvalue ?
 def: n x n 크기의 A martix가 있을때, AK = λK를 만족하는 K가 있을때, K를 eigenvector라고 하며, 숫자 λ를 eigenvalue라고 한다.
  * AK = λK, AK-λK =0, (A-λI)K=0 으로 묶을수 있고, (A-λ)의 역행렬이 존재한다면, K=0이 될수 있으므로 K는 0이 아니라는 가정이 들어간다. (그래프로 그려놓고 이해하면 쉽다)
  * 그렇다면 기하학적으로 생각해보면 어떠한 선형변환 A가 있을때, 크기만 변하고 방향이 변하지 벡터에 이용가능하며 얼마나 변한지에 대해서 λ를 이용해서 확인이 가능하다.
  
* Sampling & Resampling?
   * Sampling은 크게보면 비복원 추출법과, 복원 추출법이 있으며 대표적으로 jackknifing, bootstrapping이 대표적이다.  
     * 복원추출은 한번 시행한 결과를 다시 얻을 수 있도록 모집단에 다시 포함시켜 시행하는 추출방법이여서 같은 원소가 계속 뽑힐수 있는 점이다.
     * 비복원 추출은 한번 시행한 결과를 다시 모집단에 포함하지 않고 시행하는 추출방법이다.
   * Resampling은 표본을 추출하면서 원래 데이터 셋을 복원할 수 있는데, 이를 통해 가정도 필요 없이 표본만으로 추론이 가능하고 통계학적으로 신뢰 구간을 추론할 수 있다. 즉, 모집단 데이터에 가정이나 정보 없이도 분포 성질을 분석할 수 있다는 점!  
   
  
* Bagging & Bootstrapping?
   * bagging의 경우는 bootstrap aggregation의 줄임말로, 주어진 데이터에 대해서 bootstrap 자료를 생성하고 각 bootstrap된 자료를 각 모델링에 대해 나온 결과를 최종 결과물로 산출하는 방법이다. 일반적으로 예측 모형의 변동성이 큰 경우, 변동성 감소를 위해 사용한다.
   * Bootstrapping은 통계학에서 가설 검증을 하거나 평가지표를 계산하기 전에 랜덤 샘플링을 적용하는 방법이다. (중복허용)  
     
       
* 확률 모형 & 확률 변수?
   * 확률 변수는 확률로 표현하기위해 sample space를 정의하는 것. 다양하게 정의가 가능하므로, 일반적으로 변수라는 말을 사용하며 0~1사이의 값으로 매핑 시키는 함수를 확률 함수라고 표현한다.
   * 확률 모형은 불확실성을 확률로써 계량화하기 위해 수학적으로 만든 모형이 바로 확률모형입니다. 그리고 함수에 쓰인 계수들을 모수(parameter)라고 부릅니다. 통계학에서 모수(parameter)를 추정한다는 말을 표현하는데, 데이터 분포를 알고자하는것이 모수를 추정하는것이다.  
  
  
* 확률밀도함수(Probability Density Function)
  * PDF라고 약자로 많이 사용하며, 확률밀도함수는 연속적인 변수에 의한 확률 분포 함수를 의미한다.
  * 특정구간이 다른 구간에 비해 상대적으로 얼마나 높은가를 나타낸 형식인데, 항상 양의 값을 가지며, 모든 PDF를 합하면 1이된다는 특성을 가지고 있다.
  * 어떠한 일정 범위의 확률은 구간 PDF 넓이 즉, 적분값이 된다.  
  $$
    \displaystyle \int_\infty^\infty f\left(x\right)dx
  $$
  
* 누적분포함수(Cumulative Distribution Function)
  * CDF라고 약자로 많이 사용하며, 어떠한 확률분포에 대해서 확률변수가 특정값보다 작거나 같은 확률을 나타낸다.
  * CDF를 미분하면 PDF가 나오며, 반대로 PDF를 적분하면 CDF가 나온다.  

* 조건부확률(conditional probability)
  * 어떤 사건 B가 일어났을 때 사건 A가 일어날 확률을 의미한다.
  * 수식적으로 이해보다는 문제를 보면서 어떤 형식으로 풀이가되는지 이해하면 이해가 잘된다.
  
  
* 공분산(covariance)
  * 어떠한 확률변수 x가 있을때 보통 분포를 나타낼때 쓰는 것이 평균(중간부분), 분산(분포가 얼마나 퍼져있는가)을 이용한다.
  * 확률변수가 2가지일때는 x축을 평균, y축을 평균을 두고 그리면 어느정도 분포가 주로 모여있는지가 나오게 되는데,
  * 변수가 2개일경우, 퍼져있는 정도를 알고 싶을때 나타내는 것이 공분산을 이용한다. 만약 공분산이 0 이라면 두 변수는 서로 independent하다.
  * Cov(x,y) = E((x-u)(y-v)) 로 구할 수 있다. (u,v 는 각 각 E(X)=x의 기대값, E(Y)=y의 기대값이다.)
  * 즉, x의 편차와 y의 편차 곱의 평균이라는 뜻
  
* 상관계수(correlation)
  * 공분산에는 x,y의 단위의 크기에 영향을 받는데 (큰 값일때 큰값, 작은 값일때 작은값이 나와서) 이를 보완하기 위해 상관계수가 등장.
  * 확률변수의 절대적 크기에 영향을 받지 않도록 정규화 시켰다고 생각하면 편하다.
  * p = cov(x,y) / Root(var(x)var(y)) 로 나타낼 수 있다.
  * 1을 넘을 수 없고, x,y가 independent하다면 상관계수는 0이고, linear 하다면 1,-1이다.
  
  
* 확률분포 (common probability distributions)

  * 베르누이분포(bernoulli distribution) : 매 시행시 오직 두 가지의 가능한 결과만 일어난다고 할때, 0과 1로 결정되는 확률변수 X에 대해서
  P(X=0) = p, P(X=1) = q 일때 0<=p<=1, q = 1-p를 만족하는 확률분포이며, 이항 분포의 특수한 사례에 속한다.
  
  * 이항분포(binomial distribution) : 고등학교때 '독립시행의 확률'이라는 제목으로 가물가물하게 배운거 같은데, 어떤 시행의 결과가 True, False라고 할때, True=p로 False(q)=1-p 이고, n번 반복했을 때 나타나는 확률분포는 '이항 분포'라고 하며, 성공횟수는 베르누이 시행을 n번 하므로 n개의 베르누이 확률변수의 합과 같으므로 이항 분포의 특수한 사례가 베르누이 분포가 된다.
  
  * 균일분포(uniform distribution) : 균등분포라고도 표현하고, 모든 확률변수에 대해 균일한 확률은 갖는다. 확률변수 X가 [a,b]내의 모든 영역에서 일정한 확률을 가질때 이 확률변수 X를 균일확률변수라고 부른다.
  
  * 푸아송 분포(poisson distribution) : 단위 시간 안에 어떤 사건이 몇 번 발생할 것인지 표현하는 이산 확률 분포이다. 이산 확률 분포는 이산 확률 변수가 가지는 확률 분포를 의미하는데, 이산 확률 변수라는 말은 확률 변수가 가질 수 있는 어떤 갯수가 정해져 있다는 것이다. 
  http://blog.naver.com/PostView.nhn?blogId=mykepzzang&logNo=220840724901 여기 설명이 잘되어 있는거 같다.
  
  * 가우시안 분포(gaussian distribution) : 정규분포(standard distribution)으로 잘 알려져있는데, 수집된 자료의 분포를 근사하는 데 자주 사용되며, 중심극한정리에 의해 독립적인 확률변수들의 평균은 정규분포에 가까워지는 성질이 있다. 평균이 0 이고 표준편차가 1이면 표준정규분포라고 부른다.
  
* 중심극한정리(central limit theorem) : 동일한 확률분포를 가진 독립 확률 변수 n개의 평균의 분포는 n이 크다면 정규분포에 가까워진다는 정리이다. 또한 중심극한정리를 사용하면 모집단의 확률분포와 무관하여 모집단이 이항분포나 포아송 분포를 따르건 상관없이 그 모집단에서 추출된 표본의 평균의 분포는 표본의 크기가 증가함에 따라서 항상 정규분포에 가까워진다. 일반적으로 5면 된다고하지만 최악 50 정도 되야할 수 있지만, 현실세계는 너무 복잡하여 이러한 증명이 다 안맞을 수 있다는 점이다. 
  
* 유의확률(p-value) : 일반적으로 0.05가 안넘으면 된다라는 말을 많이하는데, 처음부터 버릴것을 예상하는 가설(귀무가설)이 맞다는 전제하에 표본에서 실제 관측된 값이랑 같거나 극단적인 통계치가 관측될 확률인데, p 값이 매우 작다고 해서 유의미한 값을 갖는건 아니니 너무 맹신하는건 별로인거 같다.
  
* 평균(mean)과 중앙값(median) : 모집단이나 표본의 특성을 잘 나타내는 기본적인 수치로 평균은 전체 표본값을 포본의갯수로 나눠주면 되는데, 평균값이 모든 집단을 대표할 수 있는것이 아니다. 극단적으로 내 친구들 연봉이 4천만원이고 내가 1억을 번다고 가정하였을때, 평균연봉이 7천만원이 될 수 있는데 이러한 경우는 평균이 집단을 대표할 수 없다. 그래서 이상치(outlier) 값들이 많을, 표본이 가지고있는 수치를 작은 값부터 큰값까지 정렬한 후 중앙을 선택하는 방법이 중앙값이다. 그외에도 어떤 샘플이 자주 등장하는 값을 대표값으로 설정하는 최빈값(mode)도 있는데 이러한 수치로도 뭔가 대표할만한 값 찾기가 쉽지 않아 분산, 표준편차 등 여러가지 수치를 특성으로 표현하는 경우가 많다.
  
* likelihood(가능도)와 probability(확률)의 차이 : 확률은 우리가 주사위를 던지면 1이 나올 확률이 1/6인것을 알고 있다. 이처럼 사전에 알고 있는 확률이 probability이며, 일반적으로 딥러닝 머신러닝에서 많이 등장하는 likelihood는 정의가 애매모호하지만, 데이타 y가 이미 관찰된 것으로 보고 u를 변수로 보는게 likelihood인데, likelihood가 가장 많이 쓰이는 부분은 주어진 데이타에서 가장 가능성이 높은 u를 찾는것이 바로 MLE(maximum likelihood estimation)이다.
  
* 좋은 feature란 무엇이며, 좋은 feature를 뽑는 방법중 어떤 방법이 있을까? 좋은 feature란 어떤 모델에서 좋은 성능을 보여줄 수 있는 데이터들의 특징(feature)이며 일반적으로 feature selection을 통해 전체 데이터에서 subset을 원본 데이터로부터 찾는다. pca, svd, nmf 등을 이용해서 차원을 축소하여 빠르게 계산을 할 수 있는데 차원 축소(dimentionality)를 하는 이유로는 어떤 잠재 공간(latent space)은 실제 관찰 공간(observation space)보다 작을 수 있기에, 샘플기반으로 잠재 공간을 파악하는것이 차원 축소라고 부른다. outlier를 제거만 하는것이 아닌 궁극적으로 latent space를 찾는것이다.
  
* 지도학습(supervised learning), 비지도학습(unsupervised learning), 강화학습(reinforcement learning)을 설명하세요 : 큰 카테고리로는 AI로 보았을때 그안에 머신러닝이라는 범주가있다. 그 중 머신러닝이라는 개념은 데이터를 이용해서 컴퓨터를 학습시키는 방법론으로 크게 세가지 분류로 나눌 수 있는데 이 세개가 지도학습, 비지도학습, 강화학습이다. 지도학습은 데이터에 대한 label(y)과 데이터(x)가 주어진 상태에서 컴퓨터를 학습하는 방법이며 대표적 예로 딥러닝에서 cnn, rnn, ann 구조가 있다. 비지도학습은 레이블(label)이 명시적으로 주어지지않은 상태에서 컴퓨터를 학습시키는 방법이며 대표적 예로 클러스터링, autoencoder 구조가 있다. 마지막으로 강화학습은 지도학습과 비지도학습은 일반적으로 정적인 환경에서 학습을 진행했다면 강화학습은 어떤 에이전트가 주어진 환경에 대해 행동을 취하고 이로부터 보상을 얻으면서 학습을 진행하고 보상을 최대화하도록 학습이 진행하며 대표적으로 q-leanring이 있으며 물론 dqn 같은경우는 cnn구조를 가져다 쓴다.
  
* 차원의 저주(curse of dimensionality)란? 변수의 수가 늘어나 차원이 커지면서 발생하는 문제를 차원의 저주라고 부른다. 즉, 고차원 데이터일수록 그 데이터로부터 모델을 학습하기가 훨씬 어려워지고 훨씬 더 많은 양의 데이터가 필요하게되는 현상인데 현실세계에서는 데이터가 무한하지 않으므로 모델설계를 잘 해야 하고 전처리 (주성분분석 등)를 이용해서 낮춰주면 좋은 성능을 기대할 수 있다. 물론 모든 경우가 그렇진 않다.

* 오브젝트 디텍션 대표적인 논문은? Object-Detection은 여러가지 물체에 대한 분류(classification) + 물체의 위치정보를 파악하는 localization이라고 생각할 수 있습니다. 크게 regional proposal과 classification이 순차적으로 이루어지는 r-cnn계열인 r-cnn, fast r-cnn, faster r-cnn, mask-rcnn이 있고 regional proposal와 clssication이 동시에 이루어지는 yolo, ssd 계열이 있다. yolo는 v5까지 나와있고 ssd는 refinedet이 state-of-the-art한 논문인거 같다.

* Reference

  https://zzsza.github.io/data/2018/02/17/datascience-interivew-questions/

  https://www.cpuheater.com/deep-learning/deep-learning-interview-questions-and-answers/

  
