# 면접질문 모음

## 
* 표본(sample)과 모집단(population) : 표본과 모집단은 통계쪽에서 가장 기본적으로 사용되는 용어로, 표본은 샘플이라는 표현을 많이 사용한다. 즉, 샘플이라는걸 많이 사용하는것처럼 전체의 데이터에 일부분이나 결과들의 집합이라고 생각할 수 있으며 모집단은 집단의 전체를 의미하는데 연구자가 알고 싶어하는 집단 전체를 의미하고 대한민국 남자와 여자의 평균 키를 알고 싶어하면 모집단은 대한민국 모든 남자 여자의 키가되고, 각각 하나의 데이터에 대해서는 sample이 된다.
  
* 왜 eigenvector & eigenvalue ?
 def: n x n 크기의 A martix가 있을때, AK = λK를 만족하는 K가 있을때, K를 eigenvector라고 하며, 숫자 λ를 eigenvalue라고 한다.
  * AK = λK, AK-λK =0, (A-λI)K=0 으로 묶을수 있고, (A-λ)의 역행렬이 존재한다면, K=0이 될수 있으므로 K는 0이 아니라는 가정이 들어간다. (그래프로 그려놓고 이해하면 쉽다)
  * 그렇다면 기하학적으로 생각해보면 어떠한 선형변환 A가 있을때, 크기만 변하고 방향이 변하지 벡터에 이용가능하며 얼마나 변한지에 대해서 λ를 이용해서 확인이 가능하다.
  
* Sampling & Resampling?
   * Sampling은 크게보면 비복원 추출법과, 복원 추출법이 있으며 대표적으로 jackknifing, bootstrapping이 대표적이다.  
     * 복원추출은 한번 시행한 결과를 다시 얻을 수 있도록 모집단에 다시 포함시켜 시행하는 추출방법이여서 같은 원소가 계속 뽑힐수 있는 점이다.
     * 비복원 추출은 한번 시행한 결과를 다시 모집단에 포함하지 않고 시행하는 추출방법이다.
   * Resampling은 표본을 추출하면서 원래 데이터 셋을 복원할 수 있는데, 이를 통해 가정도 필요 없이 표본만으로 추론이 가능하고 통계학적으로 신뢰 구간을 추론할 수 있다. 즉, 모집단 데이터에 가정이나 정보 없이도 분포 성질을 분석할 수 있다는 점!  
   
  
* Bagging & Bootstrapping?
   * bagging의 경우는 bootstrap aggregation의 줄임말로, 주어진 데이터에 대해서 bootstrap 자료를 생성하고 각 bootstrap된 자료를 각 모델링에 대해 나온 결과를 최종 결과물로 산출하는 방법이다. 일반적으로 예측 모형의 변동성이 큰 경우, 변동성 감소를 위해 사용한다.
   * Bootstrapping은 통계학에서 가설 검증을 하거나 평가지표를 계산하기 전에 랜덤 샘플링을 적용하는 방법이다. (중복허용)  
     
       
* 확률 모형 & 확률 변수?
   * 확률 변수는 확률로 표현하기위해 sample space를 정의하는 것. 다양하게 정의가 가능하므로, 일반적으로 변수라는 말을 사용하며 0~1사이의 값으로 매핑 시키는 함수를 확률 함수라고 표현한다.
   * 확률 모형은 불확실성을 확률로써 계량화하기 위해 수학적으로 만든 모형이 바로 확률모형입니다. 그리고 함수에 쓰인 계수들을 모수(parameter)라고 부릅니다. 통계학에서 모수(parameter)를 추정한다는 말을 표현하는데, 데이터 분포를 알고자하는것이 모수를 추정하는것이다.  
  
  
* 확률밀도함수(Probability Density Function)
  * PDF라고 약자로 많이 사용하며, 확률밀도함수는 연속적인 변수에 의한 확률 분포 함수를 의미한다.
  * 특정구간이 다른 구간에 비해 상대적으로 얼마나 높은가를 나타낸 형식인데, 항상 양의 값을 가지며, 모든 PDF를 합하면 1이된다는 특성을 가지고 있다.
  * 어떠한 일정 범위의 확률은 구간 PDF 넓이 즉, 적분값이 된다.  
  $$
    \displaystyle \int_\infty^\infty f\left(x\right)dx
  $$
  
* 누적분포함수(Cumulative Distribution Function)
  * CDF라고 약자로 많이 사용하며, 어떠한 확률분포에 대해서 확률변수가 특정값보다 작거나 같은 확률을 나타낸다.
  * CDF를 미분하면 PDF가 나오며, 반대로 PDF를 적분하면 CDF가 나온다.  

* 조건부확률(conditional probability)
  * 어떤 사건 B가 일어났을 때 사건 A가 일어날 확률을 의미한다.
  * 수식적으로 이해보다는 문제를 보면서 어떤 형식으로 풀이가되는지 이해하면 이해가 잘된다.
  
  
* 공분산(covariance)
  * 어떠한 확률변수 x가 있을때 보통 분포를 나타낼때 쓰는 것이 평균(중간부분), 분산(분포가 얼마나 퍼져있는가)을 이용한다.
  * 확률변수가 2가지일때는 x축을 평균, y축을 평균을 두고 그리면 어느정도 분포가 주로 모여있는지가 나오게 되는데,
  * 변수가 2개일경우, 퍼져있는 정도를 알고 싶을때 나타내는 것이 공분산을 이용한다. 만약 공분산이 0 이라면 두 변수는 서로 independent하다.
  * Cov(x,y) = E((x-u)(y-v)) 로 구할 수 있다. (u,v 는 각 각 E(X)=x의 기대값, E(Y)=y의 기대값이다.)
  * 즉, x의 편차와 y의 편차 곱의 평균이라는 뜻
  
* 상관계수(correlation)
  * 공분산에는 x,y의 단위의 크기에 영향을 받는데 (큰 값일때 큰값, 작은 값일때 작은값이 나와서) 이를 보완하기 위해 상관계수가 등장.
  * 확률변수의 절대적 크기에 영향을 받지 않도록 정규화 시켰다고 생각하면 편하다.
  * p = cov(x,y) / Root(var(x)var(y)) 로 나타낼 수 있다.
  * 1을 넘을 수 없고, x,y가 independent하다면 상관계수는 0이고, linear 하다면 1,-1이다.
  
  
* 확률분포 (common probability distributions)

  * 베르누이분포(bernoulli distribution) : 매 시행시 오직 두 가지의 가능한 결과만 일어난다고 할때, 0과 1로 결정되는 확률변수 X에 대해서
  P(X=0) = p, P(X=1) = q 일때 0<=p<=1, q = 1-p를 만족하는 확률분포이며, 이항 분포의 특수한 사례에 속한다.
  
  * 이항분포(binomial distribution) : 고등학교때 '독립시행의 확률'이라는 제목으로 가물가물하게 배운거 같은데, 어떤 시행의 결과가 True, False라고 할때, True=p로 False(q)=1-p 이고, n번 반복했을 때 나타나는 확률분포는 '이항 분포'라고 하며, 성공횟수는 베르누이 시행을 n번 하므로 n개의 베르누이 확률변수의 합과 같으므로 이항 분포의 특수한 사례가 베르누이 분포가 된다.
  
  * 균일분포(uniform distribution) : 균등분포라고도 표현하고, 모든 확률변수에 대해 균일한 확률은 갖는다. 확률변수 X가 [a,b]내의 모든 영역에서 일정한 확률을 가질때 이 확률변수 X를 균일확률변수라고 부른다.
  
  * 푸아송 분포(poisson distribution) : 단위 시간 안에 어떤 사건이 몇 번 발생할 것인지 표현하는 이산 확률 분포이다. 이산 확률 분포는 이산 확률 변수가 가지는 확률 분포를 의미하는데, 이산 확률 변수라는 말은 확률 변수가 가질 수 있는 어떤 갯수가 정해져 있다는 것이다. 
  http://blog.naver.com/PostView.nhn?blogId=mykepzzang&logNo=220840724901 여기 설명이 잘되어 있는거 같다.
  
  * 가우시안 분포(gaussian distribution) : 정규분포(standard distribution)으로 잘 알려져있는데, 수집된 자료의 분포를 근사하는 데 자주 사용되며, 중심극한정리에 의해 독립적인 확률변수들의 평균은 정규분포에 가까워지는 성질이 있다. 평균이 0 이고 표준편차가 1이면 표준정규분포라고 부른다.
  
* 중심극한정리(central limit theorem) : 동일한 확률분포를 가진 독립 확률 변수 n개의 평균의 분포는 n이 크다면 정규분포에 가까워진다는 정리이다. 또한 중심극한정리를 사용하면 모집단의 확률분포와 무관하여 모집단이 이항분포나 포아송 분포를 따르건 상관없이 그 모집단에서 추출된 표본의 평균의 분포는 표본의 크기가 증가함에 따라서 항상 정규분포에 가까워진다. 일반적으로 5면 된다고하지만 최악 50 정도 되야할 수 있지만, 현실세계는 너무 복잡하여 이러한 증명이 다 안맞을 수 있다는 점이다. 
  
* 유의확률(p-value) : 일반적으로 0.05가 안넘으면 된다라는 말을 많이하는데, 처음부터 버릴것을 예상하는 가설(귀무가설)이 맞다는 전제하에 표본에서 실제 관측된 값이랑 같거나 극단적인 통계치가 관측될 확률인데, p 값이 매우 작다고 해서 유의미한 값을 갖는건 아니니 너무 맹신하는건 별로인거 같다.
  
* 평균(mean)과 중앙값(median) : 모집단이나 표본의 특성을 잘 나타내는 기본적인 수치로 평균은 전체 표본값을 포본의갯수로 나눠주면 되는데, 평균값이 모든 집단을 대표할 수 있는것이 아니다. 극단적으로 내 친구들 연봉이 4천만원이고 내가 1억을 번다고 가정하였을때, 평균연봉이 7천만원이 될 수 있는데 이러한 경우는 평균이 집단을 대표할 수 없다. 그래서 이상치(outlier) 값들이 많을, 표본이 가지고있는 수치를 작은 값부터 큰값까지 정렬한 후 중앙을 선택하는 방법이 중앙값이다. 그외에도 어떤 샘플이 자주 등장하는 값을 대표값으로 설정하는 최빈값(mode)도 있는데 이러한 수치로도 뭔가 대표할만한 값 찾기가 쉽지 않아 분산, 표준편차 등 여러가지 수치를 특성으로 표현하는 경우가 많다.
  
* likelihood(가능도)와 probability(확률)의 차이 : 확률은 우리가 주사위를 던지면 1이 나올 확률이 1/6인것을 알고 있다. 이처럼 사전에 알고 있는 확률이 probability이며, 일반적으로 딥러닝 머신러닝에서 많이 등장하는 likelihood는 정의가 애매모호하지만, 데이타 y가 이미 관찰된 것으로 보고 u를 변수로 보는게 likelihood인데, likelihood가 가장 많이 쓰이는 부분은 주어진 데이타에서 가장 가능성이 높은 u를 찾는것이 바로 MLE(maximum likelihood estimation)이다.
  
* 좋은 feature란 무엇이며, 좋은 feature를 뽑는 방법중 어떤 방법이 있을까? 좋은 feature란 어떤 모델에서 좋은 성능을 보여줄 수 있는 데이터들의 특징(feature)이며 일반적으로 feature selection을 통해 전체 데이터에서 subset을 원본 데이터로부터 찾는다. pca, svd, nmf 등을 이용해서 차원을 축소하여 빠르게 계산을 할 수 있는데 차원 축소(dimentionality)를 하는 이유로는 어떤 잠재 공간(latent space)은 실제 관찰 공간(observation space)보다 작을 수 있기에, 샘플기반으로 잠재 공간을 파악하는것이 차원 축소라고 부른다. outlier를 제거만 하는것이 아닌 궁극적으로 latent space를 찾는것이다.
  
* 지도학습(supervised learning), 비지도학습(unsupervised learning), 강화학습(reinforcement learning)을 설명하세요 : 큰 카테고리로는 AI로 보았을때 그안에 머신러닝이라는 범주가있다. 그 중 머신러닝이라는 개념은 데이터를 이용해서 컴퓨터를 학습시키는 방법론으로 크게 세가지 분류로 나눌 수 있는데 이 세개가 지도학습, 비지도학습, 강화학습이다. 지도학습은 데이터에 대한 label(y)과 데이터(x)가 주어진 상태에서 컴퓨터를 학습하는 방법이며 대표적 예로 딥러닝에서 cnn, rnn, ann 구조가 있다. 비지도학습은 레이블(label)이 명시적으로 주어지지않은 상태에서 컴퓨터를 학습시키는 방법이며 대표적 예로 클러스터링, autoencoder 구조가 있다. 마지막으로 강화학습은 지도학습과 비지도학습은 일반적으로 정적인 환경에서 학습을 진행했다면 강화학습은 어떤 에이전트가 주어진 환경에 대해 행동을 취하고 이로부터 보상을 얻으면서 학습을 진행하고 보상을 최대화하도록 학습이 진행하며 대표적으로 q-leanring이 있으며 물론 dqn 같은경우는 cnn구조를 가져다 쓴다.
  
* 차원의 저주(curse of dimensionality)란? 변수의 수가 늘어나 차원이 커지면서 발생하는 문제를 차원의 저주라고 부른다. 즉, 고차원 데이터일수록 그 데이터로부터 모델을 학습하기가 훨씬 어려워지고 훨씬 더 많은 양의 데이터가 필요하게되는 현상인데 현실세계에서는 데이터가 무한하지 않으므로 모델설계를 잘 해야 하고 전처리 (주성분분석 등)를 이용해서 낮춰주면 좋은 성능을 기대할 수 있다. 물론 모든 경우가 그렇진 않다.

* 오브젝트 디텍션 대표적인 논문은? Object-Detection은 여러가지 물체에 대한 분류(classification) + 물체의 위치정보를 파악하는 localization이라고 생각할 수 있습니다. 크게 regional proposal과 classification이 순차적으로 이루어지는 r-cnn계열인 r-cnn, fast r-cnn, faster r-cnn, mask-rcnn이 있고 regional proposal와 clssication이 동시에 이루어지는 yolo, ssd 계열이 있다. yolo는 v5까지 나와있고 ssd는 refinedet이 state-of-the-art한 논문인거 같다.
  
* 모델 경량화 하는방법은 ? 크게 경량 알고리즘 연구, 알고리즘 경량화 연구 두 분류로 나뉠수 있는데, 경량 알고리즘 연구에는 모델 구조 변경, cnn 필터 변경, 강화 학습을 통해 최적 모델 자동 탐색 연구가 있으며 알고리즘 경량화 연구에는 모델 압축, 전이 학습, 하드웨어 가속화, 강화 학습 기반의 최적모델 연구 AMC 로 크게 볼 수 있다. 
  
* Batch Normalization ? 딥러닝에서 layer가 deep 할수록 vanishing gradient 문제가 자주 일어난다. 그래서 bn(batch normalization)은 학습 중에 covariate shift 문제가 발생한다고 나와있는데, 이전 layer의 파마리터 변화로 인해 현재 layer의 입력 분포가 바뀌는 현상이다. 그래서 whitening 방법으로 평균 0 분산 1 로 만들수도 있지만 bp(backpropagation) 되는과정과 다르게 작동하므로 특정 파라미터가 커질 수도 있다는 문제점이 있는데, 별로 프로세스 없이 학습하면서 같이 학습되는것이 whitening과 차이점이라고 볼 수 있다. 그래서 간단하게 생각하면 평균과 분산을 구해 정규화 시키고 scale 작업과 shift을 위한 변수 알파값과 감마값을 추가되었다고 볼 수 있다. cs(covariate shift)문제도 해결하고 학습속도도 빨라졌다. 즉, 좋은 성능을 얻기위해 빠르게 수렴할수 있도록 도와준다고 생각할 수 있다.
  
* TensorFlow 1.7 이상버전에서는 Eager Execution가 되는데 무슨 기능인가 ? TensorFlow를 대화형 명령 스타일로 프로그래밍 할 수 있도록 해주는건데, 기존의 그래프 기반 방식에서 벗어나 그래프 생성 없이 연산을 즉시 실행하는 명령형 프로그래밍 환경을 구성할 수 있다. 이를 통해 모델을 디버그 할 수 있으며 불필요한 상용구도를 줄일수 있다.
  
* autoencoder란 무엇이며, 어디서 사용될 수 있을까요? input이 있을때 이 값들에 대한 feature를 학습 한 후 다시 출력한 결과의 output값이 input과 같은 unsupervised learning이며 data compression 특성이 강하기 때문에 주로 분석을 위한 classficiation에 많이 사용된다. 단독으로 사용할 경우 feature extraction으로 사용하는 경우가 많지만, 이렇게 추출한 feature를 다시 인풋으로 svm, ann 등에 넣어서 분류로 사용될 수 있다.
  
* python iterable과 iterator에 대해 설명하세요 ? Iterable 객체는 반복 가능한 객체로 list, dict, set, str 등 대표적인 타입이 있어서 for a in 객체 형식으로 쓸 수 있는것은 다 iterable하다고 표현한다. Iterator는 값을 차례대로 꺼낼 수 있는 객체인데 next() 함수를 통해서 다음 객체의 값을 꺼내올 수 있다.
  
* generator와 yield을 간략하게 설명하세요 ? generator는 iterator를 생성해주는 함수입니다. yield 키워드를 이용하면 쉽게 구현할 수 있는데, yield 키워드는 양보하는 키워드로 값을 함수 외부로 전달하면서 코드 실행 for문 등, 함수 바깥에 양보를해 현재 함수르 ㄹ잠시 중단하고 외부 코드가 실행되도록 만드는 역할을 한다.
  
* vanishing gradient란 ? 기울기값이 사라지는 문제는 튜닝하기가 어렵게 만든데, 레이어가 깊어질수록 더 자주 발생되는데, 근본적인 문제는 activation function에서 문제가 발생한다. 즉, 비선형함수를 사용할때 sigmoid를 계속 곱하면 작은값이 더 작은값으로 곱해지면서 0 에 가까워지는데, 시작 layer에 대해 매우 큰 변화가 있다고 하더라도 ouput을 크게 변화시키는 문제점이 발생한다. 이러한 문제점을 해결하기 위해, relu를 자주 사용한다.
* Reference

  https://zzsza.github.io/data/2018/02/17/datascience-interivew-questions/

  https://www.cpuheater.com/deep-learning/deep-learning-interview-questions-and-answers/

  
